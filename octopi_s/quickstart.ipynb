{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PromptLearningCLIPModel were not initialized from the model checkpoint at openai/clip-vit-large-patch14 and are newly initialized: ['text_model.VPT', 'text_model.encoder.layers.0.VPT_gamma', 'text_model.encoder.layers.1.VPT_gamma', 'text_model.encoder.layers.1.VPT_shallow', 'text_model.encoder.layers.10.VPT_gamma', 'text_model.encoder.layers.10.VPT_shallow', 'text_model.encoder.layers.11.VPT_shallow', 'text_model.encoder.layers.2.VPT_gamma', 'text_model.encoder.layers.2.VPT_shallow', 'text_model.encoder.layers.3.VPT_gamma', 'text_model.encoder.layers.3.VPT_shallow', 'text_model.encoder.layers.4.VPT_gamma', 'text_model.encoder.layers.4.VPT_shallow', 'text_model.encoder.layers.5.VPT_gamma', 'text_model.encoder.layers.5.VPT_shallow', 'text_model.encoder.layers.6.VPT_gamma', 'text_model.encoder.layers.6.VPT_shallow', 'text_model.encoder.layers.7.VPT_gamma', 'text_model.encoder.layers.7.VPT_shallow', 'text_model.encoder.layers.8.VPT_gamma', 'text_model.encoder.layers.8.VPT_shallow', 'text_model.encoder.layers.9.VPT_gamma', 'text_model.encoder.layers.9.VPT_shallow', 'vision_model.VPT', 'vision_model.VPT_sensors.gelsight', 'vision_model.VPT_sensors.gelsight17', 'vision_model.VPT_sensors.gelsightmini', 'vision_model.VPT_sensors.gelsightminidotted', 'vision_model.encoder.layers.0.VPT_gamma', 'vision_model.encoder.layers.1.VPT_gamma', 'vision_model.encoder.layers.1.VPT_shallow', 'vision_model.encoder.layers.1.VPT_shallow_sensors.gelsight', 'vision_model.encoder.layers.1.VPT_shallow_sensors.gelsight17', 'vision_model.encoder.layers.1.VPT_shallow_sensors.gelsightmini', 'vision_model.encoder.layers.1.VPT_shallow_sensors.gelsightminidotted', 'vision_model.encoder.layers.10.VPT_gamma', 'vision_model.encoder.layers.10.VPT_shallow', 'vision_model.encoder.layers.10.VPT_shallow_sensors.gelsight', 'vision_model.encoder.layers.10.VPT_shallow_sensors.gelsight17', 'vision_model.encoder.layers.10.VPT_shallow_sensors.gelsightmini', 'vision_model.encoder.layers.10.VPT_shallow_sensors.gelsightminidotted', 'vision_model.encoder.layers.11.VPT_gamma', 'vision_model.encoder.layers.11.VPT_shallow', 'vision_model.encoder.layers.11.VPT_shallow_sensors.gelsight', 'vision_model.encoder.layers.11.VPT_shallow_sensors.gelsight17', 'vision_model.encoder.layers.11.VPT_shallow_sensors.gelsightmini', 'vision_model.encoder.layers.11.VPT_shallow_sensors.gelsightminidotted', 'vision_model.encoder.layers.12.VPT_gamma', 'vision_model.encoder.layers.12.VPT_shallow', 'vision_model.encoder.layers.12.VPT_shallow_sensors.gelsight', 'vision_model.encoder.layers.12.VPT_shallow_sensors.gelsight17', 'vision_model.encoder.layers.12.VPT_shallow_sensors.gelsightmini', 'vision_model.encoder.layers.12.VPT_shallow_sensors.gelsightminidotted', 'vision_model.encoder.layers.13.VPT_gamma', 'vision_model.encoder.layers.13.VPT_shallow', 'vision_model.encoder.layers.13.VPT_shallow_sensors.gelsight', 'vision_model.encoder.layers.13.VPT_shallow_sensors.gelsight17', 'vision_model.encoder.layers.13.VPT_shallow_sensors.gelsightmini', 'vision_model.encoder.layers.13.VPT_shallow_sensors.gelsightminidotted', 'vision_model.encoder.layers.14.VPT_gamma', 'vision_model.encoder.layers.14.VPT_shallow', 'vision_model.encoder.layers.14.VPT_shallow_sensors.gelsight', 'vision_model.encoder.layers.14.VPT_shallow_sensors.gelsight17', 'vision_model.encoder.layers.14.VPT_shallow_sensors.gelsightmini', 'vision_model.encoder.layers.14.VPT_shallow_sensors.gelsightminidotted', 'vision_model.encoder.layers.15.VPT_gamma', 'vision_model.encoder.layers.15.VPT_shallow', 'vision_model.encoder.layers.15.VPT_shallow_sensors.gelsight', 'vision_model.encoder.layers.15.VPT_shallow_sensors.gelsight17', 'vision_model.encoder.layers.15.VPT_shallow_sensors.gelsightmini', 'vision_model.encoder.layers.15.VPT_shallow_sensors.gelsightminidotted', 'vision_model.encoder.layers.16.VPT_gamma', 'vision_model.encoder.layers.16.VPT_shallow', 'vision_model.encoder.layers.16.VPT_shallow_sensors.gelsight', 'vision_model.encoder.layers.16.VPT_shallow_sensors.gelsight17', 'vision_model.encoder.layers.16.VPT_shallow_sensors.gelsightmini', 'vision_model.encoder.layers.16.VPT_shallow_sensors.gelsightminidotted', 'vision_model.encoder.layers.17.VPT_gamma', 'vision_model.encoder.layers.17.VPT_shallow', 'vision_model.encoder.layers.17.VPT_shallow_sensors.gelsight', 'vision_model.encoder.layers.17.VPT_shallow_sensors.gelsight17', 'vision_model.encoder.layers.17.VPT_shallow_sensors.gelsightmini', 'vision_model.encoder.layers.17.VPT_shallow_sensors.gelsightminidotted', 'vision_model.encoder.layers.18.VPT_gamma', 'vision_model.encoder.layers.18.VPT_shallow', 'vision_model.encoder.layers.18.VPT_shallow_sensors.gelsight', 'vision_model.encoder.layers.18.VPT_shallow_sensors.gelsight17', 'vision_model.encoder.layers.18.VPT_shallow_sensors.gelsightmini', 'vision_model.encoder.layers.18.VPT_shallow_sensors.gelsightminidotted', 'vision_model.encoder.layers.19.VPT_gamma', 'vision_model.encoder.layers.19.VPT_shallow', 'vision_model.encoder.layers.19.VPT_shallow_sensors.gelsight', 'vision_model.encoder.layers.19.VPT_shallow_sensors.gelsight17', 'vision_model.encoder.layers.19.VPT_shallow_sensors.gelsightmini', 'vision_model.encoder.layers.19.VPT_shallow_sensors.gelsightminidotted', 'vision_model.encoder.layers.2.VPT_gamma', 'vision_model.encoder.layers.2.VPT_shallow', 'vision_model.encoder.layers.2.VPT_shallow_sensors.gelsight', 'vision_model.encoder.layers.2.VPT_shallow_sensors.gelsight17', 'vision_model.encoder.layers.2.VPT_shallow_sensors.gelsightmini', 'vision_model.encoder.layers.2.VPT_shallow_sensors.gelsightminidotted', 'vision_model.encoder.layers.20.VPT_gamma', 'vision_model.encoder.layers.20.VPT_shallow', 'vision_model.encoder.layers.20.VPT_shallow_sensors.gelsight', 'vision_model.encoder.layers.20.VPT_shallow_sensors.gelsight17', 'vision_model.encoder.layers.20.VPT_shallow_sensors.gelsightmini', 'vision_model.encoder.layers.20.VPT_shallow_sensors.gelsightminidotted', 'vision_model.encoder.layers.21.VPT_gamma', 'vision_model.encoder.layers.21.VPT_shallow', 'vision_model.encoder.layers.21.VPT_shallow_sensors.gelsight', 'vision_model.encoder.layers.21.VPT_shallow_sensors.gelsight17', 'vision_model.encoder.layers.21.VPT_shallow_sensors.gelsightmini', 'vision_model.encoder.layers.21.VPT_shallow_sensors.gelsightminidotted', 'vision_model.encoder.layers.22.VPT_gamma', 'vision_model.encoder.layers.22.VPT_shallow', 'vision_model.encoder.layers.22.VPT_shallow_sensors.gelsight', 'vision_model.encoder.layers.22.VPT_shallow_sensors.gelsight17', 'vision_model.encoder.layers.22.VPT_shallow_sensors.gelsightmini', 'vision_model.encoder.layers.22.VPT_shallow_sensors.gelsightminidotted', 'vision_model.encoder.layers.23.VPT_shallow', 'vision_model.encoder.layers.23.VPT_shallow_sensors.gelsight', 'vision_model.encoder.layers.23.VPT_shallow_sensors.gelsight17', 'vision_model.encoder.layers.23.VPT_shallow_sensors.gelsightmini', 'vision_model.encoder.layers.23.VPT_shallow_sensors.gelsightminidotted', 'vision_model.encoder.layers.3.VPT_gamma', 'vision_model.encoder.layers.3.VPT_shallow', 'vision_model.encoder.layers.3.VPT_shallow_sensors.gelsight', 'vision_model.encoder.layers.3.VPT_shallow_sensors.gelsight17', 'vision_model.encoder.layers.3.VPT_shallow_sensors.gelsightmini', 'vision_model.encoder.layers.3.VPT_shallow_sensors.gelsightminidotted', 'vision_model.encoder.layers.4.VPT_gamma', 'vision_model.encoder.layers.4.VPT_shallow', 'vision_model.encoder.layers.4.VPT_shallow_sensors.gelsight', 'vision_model.encoder.layers.4.VPT_shallow_sensors.gelsight17', 'vision_model.encoder.layers.4.VPT_shallow_sensors.gelsightmini', 'vision_model.encoder.layers.4.VPT_shallow_sensors.gelsightminidotted', 'vision_model.encoder.layers.5.VPT_gamma', 'vision_model.encoder.layers.5.VPT_shallow', 'vision_model.encoder.layers.5.VPT_shallow_sensors.gelsight', 'vision_model.encoder.layers.5.VPT_shallow_sensors.gelsight17', 'vision_model.encoder.layers.5.VPT_shallow_sensors.gelsightmini', 'vision_model.encoder.layers.5.VPT_shallow_sensors.gelsightminidotted', 'vision_model.encoder.layers.6.VPT_gamma', 'vision_model.encoder.layers.6.VPT_shallow', 'vision_model.encoder.layers.6.VPT_shallow_sensors.gelsight', 'vision_model.encoder.layers.6.VPT_shallow_sensors.gelsight17', 'vision_model.encoder.layers.6.VPT_shallow_sensors.gelsightmini', 'vision_model.encoder.layers.6.VPT_shallow_sensors.gelsightminidotted', 'vision_model.encoder.layers.7.VPT_gamma', 'vision_model.encoder.layers.7.VPT_shallow', 'vision_model.encoder.layers.7.VPT_shallow_sensors.gelsight', 'vision_model.encoder.layers.7.VPT_shallow_sensors.gelsight17', 'vision_model.encoder.layers.7.VPT_shallow_sensors.gelsightmini', 'vision_model.encoder.layers.7.VPT_shallow_sensors.gelsightminidotted', 'vision_model.encoder.layers.8.VPT_gamma', 'vision_model.encoder.layers.8.VPT_shallow', 'vision_model.encoder.layers.8.VPT_shallow_sensors.gelsight', 'vision_model.encoder.layers.8.VPT_shallow_sensors.gelsight17', 'vision_model.encoder.layers.8.VPT_shallow_sensors.gelsightmini', 'vision_model.encoder.layers.8.VPT_shallow_sensors.gelsightminidotted', 'vision_model.encoder.layers.9.VPT_gamma', 'vision_model.encoder.layers.9.VPT_shallow', 'vision_model.encoder.layers.9.VPT_shallow_sensors.gelsight', 'vision_model.encoder.layers.9.VPT_shallow_sensors.gelsight17', 'vision_model.encoder.layers.9.VPT_shallow_sensors.gelsightmini', 'vision_model.encoder.layers.9.VPT_shallow_sensors.gelsightminidotted']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tactile ViFi-CLIP!\n",
      "Loaded tactile adapter!\n",
      "Loaded property regression model!\n",
      "\n",
      "Loading trained tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95e2002d899d457dbf4bae419e2858b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7976fb17c2b45f18f62862d21eebe09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samson/miniconda3/envs/octopi/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:796: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tokens added to tokenizer.\n",
      "Tokenizer BOS: None, EOS: <|im_end|>, Pad: <|endoftext|>\n",
      "Loaded LLM and tokenizer!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PromptLearningCLIPModel were not initialized from the model checkpoint at openai/clip-vit-large-patch14 and are newly initialized: ['text_model.VPT', 'text_model.encoder.layers.0.VPT_gamma', 'text_model.encoder.layers.1.VPT_gamma', 'text_model.encoder.layers.1.VPT_shallow', 'text_model.encoder.layers.10.VPT_gamma', 'text_model.encoder.layers.10.VPT_shallow', 'text_model.encoder.layers.11.VPT_shallow', 'text_model.encoder.layers.2.VPT_gamma', 'text_model.encoder.layers.2.VPT_shallow', 'text_model.encoder.layers.3.VPT_gamma', 'text_model.encoder.layers.3.VPT_shallow', 'text_model.encoder.layers.4.VPT_gamma', 'text_model.encoder.layers.4.VPT_shallow', 'text_model.encoder.layers.5.VPT_gamma', 'text_model.encoder.layers.5.VPT_shallow', 'text_model.encoder.layers.6.VPT_gamma', 'text_model.encoder.layers.6.VPT_shallow', 'text_model.encoder.layers.7.VPT_gamma', 'text_model.encoder.layers.7.VPT_shallow', 'text_model.encoder.layers.8.VPT_gamma', 'text_model.encoder.layers.8.VPT_shallow', 'text_model.encoder.layers.9.VPT_gamma', 'text_model.encoder.layers.9.VPT_shallow', 'vision_model.VPT', 'vision_model.VPT_sensors.gelsight', 'vision_model.VPT_sensors.gelsight17', 'vision_model.VPT_sensors.gelsightmini', 'vision_model.VPT_sensors.gelsightminidotted', 'vision_model.encoder.layers.0.VPT_gamma', 'vision_model.encoder.layers.1.VPT_gamma', 'vision_model.encoder.layers.1.VPT_shallow', 'vision_model.encoder.layers.1.VPT_shallow_sensors.gelsight', 'vision_model.encoder.layers.1.VPT_shallow_sensors.gelsight17', 'vision_model.encoder.layers.1.VPT_shallow_sensors.gelsightmini', 'vision_model.encoder.layers.1.VPT_shallow_sensors.gelsightminidotted', 'vision_model.encoder.layers.10.VPT_gamma', 'vision_model.encoder.layers.10.VPT_shallow', 'vision_model.encoder.layers.10.VPT_shallow_sensors.gelsight', 'vision_model.encoder.layers.10.VPT_shallow_sensors.gelsight17', 'vision_model.encoder.layers.10.VPT_shallow_sensors.gelsightmini', 'vision_model.encoder.layers.10.VPT_shallow_sensors.gelsightminidotted', 'vision_model.encoder.layers.11.VPT_gamma', 'vision_model.encoder.layers.11.VPT_shallow', 'vision_model.encoder.layers.11.VPT_shallow_sensors.gelsight', 'vision_model.encoder.layers.11.VPT_shallow_sensors.gelsight17', 'vision_model.encoder.layers.11.VPT_shallow_sensors.gelsightmini', 'vision_model.encoder.layers.11.VPT_shallow_sensors.gelsightminidotted', 'vision_model.encoder.layers.12.VPT_gamma', 'vision_model.encoder.layers.12.VPT_shallow', 'vision_model.encoder.layers.12.VPT_shallow_sensors.gelsight', 'vision_model.encoder.layers.12.VPT_shallow_sensors.gelsight17', 'vision_model.encoder.layers.12.VPT_shallow_sensors.gelsightmini', 'vision_model.encoder.layers.12.VPT_shallow_sensors.gelsightminidotted', 'vision_model.encoder.layers.13.VPT_gamma', 'vision_model.encoder.layers.13.VPT_shallow', 'vision_model.encoder.layers.13.VPT_shallow_sensors.gelsight', 'vision_model.encoder.layers.13.VPT_shallow_sensors.gelsight17', 'vision_model.encoder.layers.13.VPT_shallow_sensors.gelsightmini', 'vision_model.encoder.layers.13.VPT_shallow_sensors.gelsightminidotted', 'vision_model.encoder.layers.14.VPT_gamma', 'vision_model.encoder.layers.14.VPT_shallow', 'vision_model.encoder.layers.14.VPT_shallow_sensors.gelsight', 'vision_model.encoder.layers.14.VPT_shallow_sensors.gelsight17', 'vision_model.encoder.layers.14.VPT_shallow_sensors.gelsightmini', 'vision_model.encoder.layers.14.VPT_shallow_sensors.gelsightminidotted', 'vision_model.encoder.layers.15.VPT_gamma', 'vision_model.encoder.layers.15.VPT_shallow', 'vision_model.encoder.layers.15.VPT_shallow_sensors.gelsight', 'vision_model.encoder.layers.15.VPT_shallow_sensors.gelsight17', 'vision_model.encoder.layers.15.VPT_shallow_sensors.gelsightmini', 'vision_model.encoder.layers.15.VPT_shallow_sensors.gelsightminidotted', 'vision_model.encoder.layers.16.VPT_gamma', 'vision_model.encoder.layers.16.VPT_shallow', 'vision_model.encoder.layers.16.VPT_shallow_sensors.gelsight', 'vision_model.encoder.layers.16.VPT_shallow_sensors.gelsight17', 'vision_model.encoder.layers.16.VPT_shallow_sensors.gelsightmini', 'vision_model.encoder.layers.16.VPT_shallow_sensors.gelsightminidotted', 'vision_model.encoder.layers.17.VPT_gamma', 'vision_model.encoder.layers.17.VPT_shallow', 'vision_model.encoder.layers.17.VPT_shallow_sensors.gelsight', 'vision_model.encoder.layers.17.VPT_shallow_sensors.gelsight17', 'vision_model.encoder.layers.17.VPT_shallow_sensors.gelsightmini', 'vision_model.encoder.layers.17.VPT_shallow_sensors.gelsightminidotted', 'vision_model.encoder.layers.18.VPT_gamma', 'vision_model.encoder.layers.18.VPT_shallow', 'vision_model.encoder.layers.18.VPT_shallow_sensors.gelsight', 'vision_model.encoder.layers.18.VPT_shallow_sensors.gelsight17', 'vision_model.encoder.layers.18.VPT_shallow_sensors.gelsightmini', 'vision_model.encoder.layers.18.VPT_shallow_sensors.gelsightminidotted', 'vision_model.encoder.layers.19.VPT_gamma', 'vision_model.encoder.layers.19.VPT_shallow', 'vision_model.encoder.layers.19.VPT_shallow_sensors.gelsight', 'vision_model.encoder.layers.19.VPT_shallow_sensors.gelsight17', 'vision_model.encoder.layers.19.VPT_shallow_sensors.gelsightmini', 'vision_model.encoder.layers.19.VPT_shallow_sensors.gelsightminidotted', 'vision_model.encoder.layers.2.VPT_gamma', 'vision_model.encoder.layers.2.VPT_shallow', 'vision_model.encoder.layers.2.VPT_shallow_sensors.gelsight', 'vision_model.encoder.layers.2.VPT_shallow_sensors.gelsight17', 'vision_model.encoder.layers.2.VPT_shallow_sensors.gelsightmini', 'vision_model.encoder.layers.2.VPT_shallow_sensors.gelsightminidotted', 'vision_model.encoder.layers.20.VPT_gamma', 'vision_model.encoder.layers.20.VPT_shallow', 'vision_model.encoder.layers.20.VPT_shallow_sensors.gelsight', 'vision_model.encoder.layers.20.VPT_shallow_sensors.gelsight17', 'vision_model.encoder.layers.20.VPT_shallow_sensors.gelsightmini', 'vision_model.encoder.layers.20.VPT_shallow_sensors.gelsightminidotted', 'vision_model.encoder.layers.21.VPT_gamma', 'vision_model.encoder.layers.21.VPT_shallow', 'vision_model.encoder.layers.21.VPT_shallow_sensors.gelsight', 'vision_model.encoder.layers.21.VPT_shallow_sensors.gelsight17', 'vision_model.encoder.layers.21.VPT_shallow_sensors.gelsightmini', 'vision_model.encoder.layers.21.VPT_shallow_sensors.gelsightminidotted', 'vision_model.encoder.layers.22.VPT_gamma', 'vision_model.encoder.layers.22.VPT_shallow', 'vision_model.encoder.layers.22.VPT_shallow_sensors.gelsight', 'vision_model.encoder.layers.22.VPT_shallow_sensors.gelsight17', 'vision_model.encoder.layers.22.VPT_shallow_sensors.gelsightmini', 'vision_model.encoder.layers.22.VPT_shallow_sensors.gelsightminidotted', 'vision_model.encoder.layers.23.VPT_shallow', 'vision_model.encoder.layers.23.VPT_shallow_sensors.gelsight', 'vision_model.encoder.layers.23.VPT_shallow_sensors.gelsight17', 'vision_model.encoder.layers.23.VPT_shallow_sensors.gelsightmini', 'vision_model.encoder.layers.23.VPT_shallow_sensors.gelsightminidotted', 'vision_model.encoder.layers.3.VPT_gamma', 'vision_model.encoder.layers.3.VPT_shallow', 'vision_model.encoder.layers.3.VPT_shallow_sensors.gelsight', 'vision_model.encoder.layers.3.VPT_shallow_sensors.gelsight17', 'vision_model.encoder.layers.3.VPT_shallow_sensors.gelsightmini', 'vision_model.encoder.layers.3.VPT_shallow_sensors.gelsightminidotted', 'vision_model.encoder.layers.4.VPT_gamma', 'vision_model.encoder.layers.4.VPT_shallow', 'vision_model.encoder.layers.4.VPT_shallow_sensors.gelsight', 'vision_model.encoder.layers.4.VPT_shallow_sensors.gelsight17', 'vision_model.encoder.layers.4.VPT_shallow_sensors.gelsightmini', 'vision_model.encoder.layers.4.VPT_shallow_sensors.gelsightminidotted', 'vision_model.encoder.layers.5.VPT_gamma', 'vision_model.encoder.layers.5.VPT_shallow', 'vision_model.encoder.layers.5.VPT_shallow_sensors.gelsight', 'vision_model.encoder.layers.5.VPT_shallow_sensors.gelsight17', 'vision_model.encoder.layers.5.VPT_shallow_sensors.gelsightmini', 'vision_model.encoder.layers.5.VPT_shallow_sensors.gelsightminidotted', 'vision_model.encoder.layers.6.VPT_gamma', 'vision_model.encoder.layers.6.VPT_shallow', 'vision_model.encoder.layers.6.VPT_shallow_sensors.gelsight', 'vision_model.encoder.layers.6.VPT_shallow_sensors.gelsight17', 'vision_model.encoder.layers.6.VPT_shallow_sensors.gelsightmini', 'vision_model.encoder.layers.6.VPT_shallow_sensors.gelsightminidotted', 'vision_model.encoder.layers.7.VPT_gamma', 'vision_model.encoder.layers.7.VPT_shallow', 'vision_model.encoder.layers.7.VPT_shallow_sensors.gelsight', 'vision_model.encoder.layers.7.VPT_shallow_sensors.gelsight17', 'vision_model.encoder.layers.7.VPT_shallow_sensors.gelsightmini', 'vision_model.encoder.layers.7.VPT_shallow_sensors.gelsightminidotted', 'vision_model.encoder.layers.8.VPT_gamma', 'vision_model.encoder.layers.8.VPT_shallow', 'vision_model.encoder.layers.8.VPT_shallow_sensors.gelsight', 'vision_model.encoder.layers.8.VPT_shallow_sensors.gelsight17', 'vision_model.encoder.layers.8.VPT_shallow_sensors.gelsightmini', 'vision_model.encoder.layers.8.VPT_shallow_sensors.gelsightminidotted', 'vision_model.encoder.layers.9.VPT_gamma', 'vision_model.encoder.layers.9.VPT_shallow', 'vision_model.encoder.layers.9.VPT_shallow_sensors.gelsight', 'vision_model.encoder.layers.9.VPT_shallow_sensors.gelsight17', 'vision_model.encoder.layers.9.VPT_shallow_sensors.gelsightmini', 'vision_model.encoder.layers.9.VPT_shallow_sensors.gelsightminidotted']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tactile ViFi-CLIP!\n",
      "Loaded tactile adapter!\n",
      "Loaded property regression model!\n",
      "\n",
      "Loaded trained projection module!\n",
      "Loaded model in 40.922105 seconds.\n"
     ]
    }
   ],
   "source": [
    "import yaml, os\n",
    "import torch\n",
    "import cv2 as cv\n",
    "from utils.encoder import *\n",
    "from utils.llm import *\n",
    "from utils.dataset import *\n",
    "from transformers import CLIPImageProcessor, AutoProcessor\n",
    "from transformers.utils import logging\n",
    "from utils.demo_utils import *\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.dataset import get_frames\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "# Run settings\n",
    "run_type = f\"demo\"\n",
    "demo_config_path = f'../configs/{run_type}.yaml'\n",
    "demo_configs = yaml.safe_load(open(demo_config_path))\n",
    "device = f'cuda:{demo_configs[\"cuda\"]}'\n",
    "load_exp_path = demo_configs[\"load_exp_path\"]\n",
    "f = open(demo_configs[\"gpu_config\"])\n",
    "gpu_config = json.load(f)\n",
    "demo_path = demo_configs[\"demo_path\"]\n",
    "chat_path = demo_configs[\"chat_path\"]\n",
    "\n",
    "# RAG\n",
    "tactile_vificlip, tactile_adapter, property_classifier, load_exp_configs = load_encoder(demo_configs, device)\n",
    "image_transforms = get_image_transforms(load_exp_configs[\"frame_size\"], \"physiclear\", split_name=\"test\", flip_p=0) # NOTE: Assume the tactile inputs uses non-dotted GelSight Mini\n",
    "if demo_configs[\"rag\"]:\n",
    "    if demo_configs[\"rag_generate_embeddings\"]:\n",
    "        print(\"\\nGenerating RAG embeddings...\")\n",
    "        generate_rag_embeddings(demo_configs, load_exp_configs, tactile_vificlip, device, demo_configs[\"rag_sample_dir\"], demo_configs[\"embedding_dir\"])\n",
    "    del tactile_adapter\n",
    "    del property_classifier\n",
    "    saved_embeddings, sample_tactile_paths, rag_object_ids = get_rag_embeddings(demo_configs, device)\n",
    "else:\n",
    "    tactile_vificlip = None\n",
    "    saved_embeddings = None\n",
    "    sample_tactile_paths = None\n",
    "    rag_object_ids = None\n",
    "\n",
    "# Load models\n",
    "load_exp_configs = yaml.safe_load(open(os.path.join(load_exp_path, \"run.yaml\")))\n",
    "peft = load_exp_configs[\"peft\"]\n",
    "tokenizer_path, model_path, new_tokens, no_split_module_classes = get_model_details(load_exp_configs[\"model_type\"])\n",
    "load_exp_configs.update(demo_configs)\n",
    "start = datetime.now()\n",
    "model = load_mllm(load_exp_configs, tokenizer_path, model_path, new_tokens, no_split_module_classes, peft, device, gpu_config, exp_id=None)\n",
    "if load_exp_configs[\"use_clip\"]:\n",
    "    image_processor = CLIPImageProcessor.from_pretrained(load_exp_configs[\"use_clip\"])\n",
    "end = datetime.now()\n",
    "elapsed = (end - start).total_seconds()\n",
    "print(f\"Loaded model in {elapsed} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_frames_inline(sample_path, image_transforms):\n",
    "    plt.rcParams['figure.dpi'] = 200\n",
    "    rows = 1\n",
    "    columns = 10\n",
    "    f, arr = plt.subplots(rows, columns)\n",
    "    mean, std = get_dataset_img_norm(\"physiclear\") # NOTE: Assume the tactile inputs uses non-dotted GelSight Mini\n",
    "    inverse_transforms_list = [\n",
    "        transforms.Normalize(\n",
    "            mean=[-mean[0]/std[0], -mean[1]/std[1], -mean[2]/std[2]],\n",
    "            std=[1/std[0], 1/std[1], 1/std[2]]\n",
    "        ),\n",
    "    ]\n",
    "    inverse_transforms = transforms.Compose(inverse_transforms_list)\n",
    "    image_tensors = inverse_transforms(get_frames(sample_path, None, image_transforms, frame_size=load_exp_configs[\"frame_size\"], train=False))\n",
    "    padding_size = rows * columns - image_tensors.shape[0]\n",
    "    if padding_size > 0:\n",
    "        padding = torch.stack([image_tensors[-1]] * padding_size, dim=0)\n",
    "        image_tensors = torch.cat([image_tensors, padding], dim=0)\n",
    "    for i in range(len(image_tensors)):\n",
    "        arr[int(i%columns)].axis('off')\n",
    "        arr[int(i%columns)].imshow(image_tensors[i].cpu().numpy().transpose(1,2,0))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def get_all_embeds(user_input, prev_embeds, configs, tokenizer):\n",
    "    if \"$\" in user_input:\n",
    "        user_input = user_input.strip()\n",
    "        if \"$dr\" in user_input:\n",
    "            describe = True\n",
    "            rank = True\n",
    "        elif \"$d\" in user_input:\n",
    "            describe = True\n",
    "            rank = False\n",
    "        elif \"$r\" in user_input:\n",
    "            describe = False\n",
    "            rank = True\n",
    "        object_ids = user_input.split(\"(\")[-1].replace(\")\", \"\")\n",
    "        object_ids = [int(i.strip()) for i in object_ids.split(\",\")]\n",
    "        generation, all_embeds, question, tactile_paths_flattened = describe_rank(model, tactile_vificlip, demo_configs, load_exp_configs, object_ids, image_transforms, device, image_processor, new_tokens, saved_embeddings, sample_tactile_paths, rag_object_ids, prev_embeds, describe, rank)\n",
    "        question = question.replace(\"]\", \"[\").split(\"[\") # NOTE: Assume the tactile inputs uses the non-dotted GelSight Mini\n",
    "        print(f\"###### USER: {question[0]}\\n\", flush=True)\n",
    "        tactile_count = 0\n",
    "        for chunk in question[1:]:\n",
    "            if \"frames\" in chunk:\n",
    "                plot_frames_inline(tactile_paths_flattened[tactile_count], image_transforms)\n",
    "                tactile_count += 1\n",
    "            else:\n",
    "                print(chunk, flush=True)\n",
    "    else:\n",
    "        print(f\"###### USER: {user_input}\\n\")\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ]\n",
    "        question_template = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        question_embeds = process_user_input(question_template, image_processor, model, tokenizer, device, new_tokens, configs[\"frame_size\"], image_transforms)\n",
    "        generation, generation_embeds, question_embeds = generate(question_embeds, model, demo_configs[\"max_new_tokens\"], prev_embeds=prev_embeds)\n",
    "        all_embeds = torch.cat([question_embeds, generation_embeds], dim=1)\n",
    "    return all_embeds, generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### USER: exit\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### ASSISTANT: I'm sorry, but I'm not sure what you mean by \"exit\". Could you please provide more context or clarify your question?<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "with torch.no_grad():\n",
    "    user_input = input(f\"USER: \")\n",
    "    while user_input == \"restart\":\n",
    "        clear_output(wait=True)\n",
    "        user_input = input(f\"USER: \")\n",
    "    all_embeds = None\n",
    "    all_embeds, generation = get_all_embeds(user_input, all_embeds, load_exp_configs, model.tokenizer)\n",
    "    print(f\"###### ASSISTANT: {generation}\\n\", flush=True)\n",
    "    user_input = input(\"USER: \")\n",
    "\n",
    "    while user_input.strip() != \"exit\":\n",
    "        if user_input == \"restart\":\n",
    "            clear_output(wait=True)\n",
    "            all_embeds = None\n",
    "            user_input = input(f\"USER: \")\n",
    "        elif len(user_input) == 0:\n",
    "            user_input = input(f\"USER: \")\n",
    "        else:\n",
    "            all_embeds, generation = get_all_embeds(user_input, all_embeds, load_exp_configs, model.tokenizer)\n",
    "            print(f\"###### ASSISTANT: {generation}\\n\", flush=True)\n",
    "            user_input = input(\"USER: \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "octopi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
