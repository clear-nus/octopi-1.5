{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PromptLearningCLIPModel were not initialized from the model checkpoint at openai/clip-vit-large-patch14 and are newly initialized: ['text_model.VPT', 'text_model.encoder.layers.0.VPT_gamma', 'text_model.encoder.layers.1.VPT_gamma', 'text_model.encoder.layers.1.VPT_shallow', 'text_model.encoder.layers.10.VPT_gamma', 'text_model.encoder.layers.10.VPT_shallow', 'text_model.encoder.layers.11.VPT_shallow', 'text_model.encoder.layers.2.VPT_gamma', 'text_model.encoder.layers.2.VPT_shallow', 'text_model.encoder.layers.3.VPT_gamma', 'text_model.encoder.layers.3.VPT_shallow', 'text_model.encoder.layers.4.VPT_gamma', 'text_model.encoder.layers.4.VPT_shallow', 'text_model.encoder.layers.5.VPT_gamma', 'text_model.encoder.layers.5.VPT_shallow', 'text_model.encoder.layers.6.VPT_gamma', 'text_model.encoder.layers.6.VPT_shallow', 'text_model.encoder.layers.7.VPT_gamma', 'text_model.encoder.layers.7.VPT_shallow', 'text_model.encoder.layers.8.VPT_gamma', 'text_model.encoder.layers.8.VPT_shallow', 'text_model.encoder.layers.9.VPT_gamma', 'text_model.encoder.layers.9.VPT_shallow', 'vision_model.VPT', 'vision_model.encoder.layers.0.VPT_gamma', 'vision_model.encoder.layers.1.VPT_gamma', 'vision_model.encoder.layers.1.VPT_shallow', 'vision_model.encoder.layers.10.VPT_gamma', 'vision_model.encoder.layers.10.VPT_shallow', 'vision_model.encoder.layers.11.VPT_gamma', 'vision_model.encoder.layers.11.VPT_shallow', 'vision_model.encoder.layers.12.VPT_gamma', 'vision_model.encoder.layers.12.VPT_shallow', 'vision_model.encoder.layers.13.VPT_gamma', 'vision_model.encoder.layers.13.VPT_shallow', 'vision_model.encoder.layers.14.VPT_gamma', 'vision_model.encoder.layers.14.VPT_shallow', 'vision_model.encoder.layers.15.VPT_gamma', 'vision_model.encoder.layers.15.VPT_shallow', 'vision_model.encoder.layers.16.VPT_gamma', 'vision_model.encoder.layers.16.VPT_shallow', 'vision_model.encoder.layers.17.VPT_gamma', 'vision_model.encoder.layers.17.VPT_shallow', 'vision_model.encoder.layers.18.VPT_gamma', 'vision_model.encoder.layers.18.VPT_shallow', 'vision_model.encoder.layers.19.VPT_gamma', 'vision_model.encoder.layers.19.VPT_shallow', 'vision_model.encoder.layers.2.VPT_gamma', 'vision_model.encoder.layers.2.VPT_shallow', 'vision_model.encoder.layers.20.VPT_gamma', 'vision_model.encoder.layers.20.VPT_shallow', 'vision_model.encoder.layers.21.VPT_gamma', 'vision_model.encoder.layers.21.VPT_shallow', 'vision_model.encoder.layers.22.VPT_gamma', 'vision_model.encoder.layers.22.VPT_shallow', 'vision_model.encoder.layers.23.VPT_shallow', 'vision_model.encoder.layers.3.VPT_gamma', 'vision_model.encoder.layers.3.VPT_shallow', 'vision_model.encoder.layers.4.VPT_gamma', 'vision_model.encoder.layers.4.VPT_shallow', 'vision_model.encoder.layers.5.VPT_gamma', 'vision_model.encoder.layers.5.VPT_shallow', 'vision_model.encoder.layers.6.VPT_gamma', 'vision_model.encoder.layers.6.VPT_shallow', 'vision_model.encoder.layers.7.VPT_gamma', 'vision_model.encoder.layers.7.VPT_shallow', 'vision_model.encoder.layers.8.VPT_gamma', 'vision_model.encoder.layers.8.VPT_shallow', 'vision_model.encoder.layers.9.VPT_gamma', 'vision_model.encoder.layers.9.VPT_shallow']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tactile ViFi-CLIP!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/Documents/octopi-s/octopi_s/utils/encoder.py:524: UserWarning: No trained dotted tactile adapter found!\n",
      "  warnings.warn(\"No trained dotted tactile adapter found!\")\n",
      "/home/user/Documents/octopi-s/octopi_s/utils/encoder.py:532: UserWarning: No trained dotted tactile adapter found!\n",
      "  warnings.warn(\"No trained dotted tactile adapter found!\")\n",
      "/home/user/Documents/octopi-s/octopi_s/utils/encoder.py:540: UserWarning: No trained property regression model found!\n",
      "  warnings.warn(\"No trained property regression model found!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained tokenizer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98e3eceb258f48269e2002f6ec294547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86d761e4d9454965a70aadd22b3698f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.conda/envs/octopis/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:796: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded LLM and tokenizer!\n",
      "0 tokens added to tokenizer.\n",
      "Tokenizer BOS: None, EOS: <|im_end|>, Pad: <|endoftext|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.conda/envs/octopis/lib/python3.8/site-packages/peft/utils/save_and_load.py:198: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  adapters_weights = torch.load(filename, map_location=torch.device(device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded trained PEFT LLM!\n",
      "Loaded LLM!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PromptLearningCLIPModel were not initialized from the model checkpoint at openai/clip-vit-large-patch14 and are newly initialized: ['text_model.VPT', 'text_model.encoder.layers.0.VPT_gamma', 'text_model.encoder.layers.1.VPT_gamma', 'text_model.encoder.layers.1.VPT_shallow', 'text_model.encoder.layers.10.VPT_gamma', 'text_model.encoder.layers.10.VPT_shallow', 'text_model.encoder.layers.11.VPT_shallow', 'text_model.encoder.layers.2.VPT_gamma', 'text_model.encoder.layers.2.VPT_shallow', 'text_model.encoder.layers.3.VPT_gamma', 'text_model.encoder.layers.3.VPT_shallow', 'text_model.encoder.layers.4.VPT_gamma', 'text_model.encoder.layers.4.VPT_shallow', 'text_model.encoder.layers.5.VPT_gamma', 'text_model.encoder.layers.5.VPT_shallow', 'text_model.encoder.layers.6.VPT_gamma', 'text_model.encoder.layers.6.VPT_shallow', 'text_model.encoder.layers.7.VPT_gamma', 'text_model.encoder.layers.7.VPT_shallow', 'text_model.encoder.layers.8.VPT_gamma', 'text_model.encoder.layers.8.VPT_shallow', 'text_model.encoder.layers.9.VPT_gamma', 'text_model.encoder.layers.9.VPT_shallow', 'vision_model.VPT', 'vision_model.encoder.layers.0.VPT_gamma', 'vision_model.encoder.layers.1.VPT_gamma', 'vision_model.encoder.layers.1.VPT_shallow', 'vision_model.encoder.layers.10.VPT_gamma', 'vision_model.encoder.layers.10.VPT_shallow', 'vision_model.encoder.layers.11.VPT_gamma', 'vision_model.encoder.layers.11.VPT_shallow', 'vision_model.encoder.layers.12.VPT_gamma', 'vision_model.encoder.layers.12.VPT_shallow', 'vision_model.encoder.layers.13.VPT_gamma', 'vision_model.encoder.layers.13.VPT_shallow', 'vision_model.encoder.layers.14.VPT_gamma', 'vision_model.encoder.layers.14.VPT_shallow', 'vision_model.encoder.layers.15.VPT_gamma', 'vision_model.encoder.layers.15.VPT_shallow', 'vision_model.encoder.layers.16.VPT_gamma', 'vision_model.encoder.layers.16.VPT_shallow', 'vision_model.encoder.layers.17.VPT_gamma', 'vision_model.encoder.layers.17.VPT_shallow', 'vision_model.encoder.layers.18.VPT_gamma', 'vision_model.encoder.layers.18.VPT_shallow', 'vision_model.encoder.layers.19.VPT_gamma', 'vision_model.encoder.layers.19.VPT_shallow', 'vision_model.encoder.layers.2.VPT_gamma', 'vision_model.encoder.layers.2.VPT_shallow', 'vision_model.encoder.layers.20.VPT_gamma', 'vision_model.encoder.layers.20.VPT_shallow', 'vision_model.encoder.layers.21.VPT_gamma', 'vision_model.encoder.layers.21.VPT_shallow', 'vision_model.encoder.layers.22.VPT_gamma', 'vision_model.encoder.layers.22.VPT_shallow', 'vision_model.encoder.layers.23.VPT_shallow', 'vision_model.encoder.layers.3.VPT_gamma', 'vision_model.encoder.layers.3.VPT_shallow', 'vision_model.encoder.layers.4.VPT_gamma', 'vision_model.encoder.layers.4.VPT_shallow', 'vision_model.encoder.layers.5.VPT_gamma', 'vision_model.encoder.layers.5.VPT_shallow', 'vision_model.encoder.layers.6.VPT_gamma', 'vision_model.encoder.layers.6.VPT_shallow', 'vision_model.encoder.layers.7.VPT_gamma', 'vision_model.encoder.layers.7.VPT_shallow', 'vision_model.encoder.layers.8.VPT_gamma', 'vision_model.encoder.layers.8.VPT_shallow', 'vision_model.encoder.layers.9.VPT_gamma', 'vision_model.encoder.layers.9.VPT_shallow']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tactile ViFi-CLIP!\n",
      "\n",
      "Loaded projection module!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/Documents/octopi-s/octopi_s/utils/encoder.py:524: UserWarning: No trained dotted tactile adapter found!\n",
      "  warnings.warn(\"No trained dotted tactile adapter found!\")\n",
      "/home/user/Documents/octopi-s/octopi_s/utils/encoder.py:532: UserWarning: No trained dotted tactile adapter found!\n",
      "  warnings.warn(\"No trained dotted tactile adapter found!\")\n",
      "/home/user/Documents/octopi-s/octopi_s/utils/encoder.py:540: UserWarning: No trained property regression model found!\n",
      "  warnings.warn(\"No trained property regression model found!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model in 68.829394 seconds.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2 as cv\n",
    "import yaml, os\n",
    "import torch, os, yaml\n",
    "from utils.encoder import *\n",
    "from utils.llm import *\n",
    "from utils.dataset import *\n",
    "from transformers import CLIPImageProcessor, AutoProcessor\n",
    "from transformers.utils import logging\n",
    "from utils.demo_utils import *\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.dataset import get_frames\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "# Run settings\n",
    "run_type = f\"demo\"\n",
    "demo_config_path = f'../configs/{run_type}.yaml'\n",
    "demo_configs = yaml.safe_load(open(demo_config_path))\n",
    "device = f'cuda:{demo_configs[\"cuda\"]}'\n",
    "load_exp_path = demo_configs[\"load_exp_path\"]\n",
    "f = open(demo_configs[\"gpu_config\"])\n",
    "gpu_config = json.load(f)\n",
    "demo_path = demo_configs[\"demo_path\"]\n",
    "chat_path = demo_configs[\"chat_path\"]\n",
    "dataset = \"physiclear\" # NOTE: Assume the tactile inputs uses non-dotted GelSight Mini\n",
    "\n",
    "# RAG\n",
    "tactile_vificlip, dotted_tactile_adapter, plain_tactile_adapter, property_classifier, load_exp_configs = load_encoder(demo_configs, device)\n",
    "image_transforms = get_image_transforms(load_exp_configs[\"frame_size\"], dataset, split_name=\"test\", flip_p=0)\n",
    "if demo_configs[\"rag\"]:\n",
    "    if demo_configs[\"rag_generate_embeddings\"]:\n",
    "        print(\"\\nGenerating RAG embeddings...\")\n",
    "        generate_rag_embeddings(demo_configs, load_exp_configs, tactile_vificlip, device, demo_configs[\"rag_sample_dir\"], demo_configs[\"embedding_dir\"])\n",
    "    del dotted_tactile_adapter\n",
    "    del plain_tactile_adapter\n",
    "    del property_classifier\n",
    "    saved_embeddings, sample_tactile_paths, rag_object_ids = get_rag_embeddings(demo_configs, device)\n",
    "else:\n",
    "    tactile_vificlip = None\n",
    "    saved_embeddings = None\n",
    "    sample_tactile_paths = None\n",
    "    object_ids = None\n",
    "\n",
    "# Load models\n",
    "load_exp_configs = yaml.safe_load(open(os.path.join(load_exp_path, \"run.yaml\")))\n",
    "peft = \"peft\" in demo_configs[\"load_exp_path\"]\n",
    "tokenizer_path, model_path, new_tokens, no_split_module_classes = get_model_details(load_exp_configs[\"model_type\"])\n",
    "load_exp_configs.update(demo_configs)\n",
    "start = datetime.now()\n",
    "model = load_mllm(load_exp_configs, tokenizer_path, model_path, new_tokens, no_split_module_classes, peft, device, gpu_config, exp_id=None)\n",
    "if load_exp_configs[\"use_clip\"]:\n",
    "    image_processor = CLIPImageProcessor.from_pretrained(load_exp_configs[\"use_clip\"])\n",
    "end = datetime.now()\n",
    "elapsed = (end - start).total_seconds()\n",
    "print(f\"Loaded model in {elapsed} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_transforms_list = [\n",
    "    transforms.Normalize(\n",
    "        mean=[-0.48145466/0.26862954, -0.4578275/0.26130258, -0.40821073/0.27577711],\n",
    "        std=[1/0.26862954, 1/0.26130258, 1/0.27577711]\n",
    "    ),\n",
    "]\n",
    "inverse_transforms = transforms.Compose(inverse_transforms_list)\n",
    "\n",
    "\n",
    "def plot_frames_inline(sample_path, image_transforms):\n",
    "    plt.rcParams['figure.dpi'] = 200\n",
    "    rows = 1\n",
    "    columns = 10\n",
    "    f, arr = plt.subplots(rows, columns)\n",
    "    # plt.suptitle(sample)\n",
    "    image_tensors = inverse_transforms(get_frames(sample_path, None, image_transforms, frame_size=load_exp_configs[\"frame_size\"], train=False))\n",
    "    padding_size = rows * columns - image_tensors.shape[0]\n",
    "    if padding_size > 0:\n",
    "        padding = torch.stack([image_tensors[-1]] * padding_size, dim=0)\n",
    "        image_tensors = torch.cat([image_tensors, padding], dim=0)\n",
    "    for i in range(len(image_tensors)):\n",
    "        arr[int(i%columns)].axis('off')\n",
    "        arr[int(i%columns)].imshow(image_tensors[i].cpu().numpy().transpose(1,2,0))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def get_all_embeds(user_input, prev_embeds, configs, tokenizer):\n",
    "    if \"$\" in user_input:\n",
    "        user_input = user_input.strip()\n",
    "        if \"$dr\" in user_input:\n",
    "            describe = True\n",
    "            rank = True\n",
    "        elif \"$d\" in user_input:\n",
    "            describe = True\n",
    "            rank = False\n",
    "        elif \"$r\" in user_input:\n",
    "            describe = False\n",
    "            rank = True\n",
    "        object_ids = user_input.split(\"(\")[-1].replace(\")\", \"\")\n",
    "        object_ids = [int(i.strip()) for i in object_ids.split(\",\")]\n",
    "        generation, all_embeds, question, tactile_paths_flattened = describe_rank(model, tactile_vificlip, demo_configs, load_exp_configs, object_ids, image_transforms, device, image_processor, new_tokens, saved_embeddings, sample_tactile_paths, rag_object_ids, prev_embeds, describe, rank)\n",
    "        question = question.replace(\"]\", \"[\").split(\"[\") # NOTE: Assume the tactile inputs uses the non-dotted GelSight Mini\n",
    "        print(f\"###### USER: {question[0]}\\n\", flush=True)\n",
    "        tactile_count = 0\n",
    "        for chunk in question[1:]:\n",
    "            if \"frames\" in chunk:\n",
    "                plot_frames_inline(tactile_paths_flattened[tactile_count], image_transforms)\n",
    "                tactile_count += 1\n",
    "            else:\n",
    "                print(chunk, flush=True)\n",
    "    else:\n",
    "        print(f\"###### USER: {user_input}\\n\")\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ]\n",
    "        question_template = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        question_embeds = process_user_input(question_template, image_processor, model, tokenizer, device, new_tokens, configs[\"frame_size\"], image_transforms)\n",
    "        generation, generation_embeds, question_embeds = generate(question_embeds, model, demo_configs[\"max_new_tokens\"], prev_embeds=prev_embeds)\n",
    "        all_embeds = torch.cat([question_embeds, generation_embeds], dim=1)\n",
    "    return all_embeds, generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### USER: hi there\n",
      "\n",
      "###### ASSISTANT: Hello! How can I help you today?<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "with torch.no_grad():\n",
    "    user_input = input(f\"USER: \")\n",
    "    while user_input == \"restart\":\n",
    "        clear_output(wait=True)\n",
    "        user_input = input(f\"USER: \")\n",
    "    all_embeds = None\n",
    "    all_embeds, generation = get_all_embeds(user_input, all_embeds, load_exp_configs, model.tokenizer)\n",
    "    print(f\"###### ASSISTANT: {generation}\\n\", flush=True)\n",
    "    user_input = input(\"USER: \")\n",
    "\n",
    "    while user_input.strip() != \"exit\":\n",
    "        if user_input == \"restart\":\n",
    "            clear_output(wait=True)\n",
    "            all_embeds = None\n",
    "            user_input = input(f\"USER: \")\n",
    "        elif len(user_input) == 0:\n",
    "            user_input = input(f\"USER: \")\n",
    "        else:\n",
    "            all_embeds, generation = get_all_embeds(user_input, all_embeds, load_exp_configs, model.tokenizer)\n",
    "            print(f\"###### ASSISTANT: {generation}\\n\", flush=True)\n",
    "            user_input = input(\"USER: \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
